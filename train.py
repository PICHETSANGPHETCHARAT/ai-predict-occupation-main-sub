import os
import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from collections import Counter
from pythainlp.tokenize import word_tokenize
from gensim.models import Word2Vec

# ЁЯФ╣ р╣Вр╕лр╕ер╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕е
print("ЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╣Вр╕лр╕ер╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕е...")
file_path = "Updated_Occupation_Titles.csv"  # ЁЯФД р╣Бр╕Бр╣Йр╣Ар╕Ыр╣Зр╕Щр╕Юр╕▓р╕Шр╣Др╕Яр╕ер╣Мр╕Вр╕нр╕Зр╕Др╕╕р╕У
df = pd.read_csv(file_path, encoding="utf-8")

# ЁЯФ╣ р╕ер╕Ър╣Бр╕Цр╕зр╕Чр╕╡р╣Ир╕бр╕╡р╕Др╣Ир╕▓ NaN р╣Бр╕ер╕░р╕нр╕▒р╕Бр╕Вр╕гр╕░р╕Юр╕┤р╣Ар╕ир╕й
df = df.dropna()
df["р╕Кр╕╖р╣Ир╕нр╕Хр╕│р╣Бр╕лр╕Щр╣Ир╕Зр╕Зр╕▓р╕Щ"] = df["р╕Кр╕╖р╣Ир╕нр╕Хр╕│р╣Бр╕лр╕Щр╣Ир╕Зр╕Зр╕▓р╕Щ"].str.replace(r"[^\w\s]", "", regex=True)

# ЁЯФ╣ р╣Бр╕кр╕Фр╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Чр╕▒р╣Ир╕зр╣Др╕Ыр╣Ар╕Бр╕╡р╣Ир╕вр╕зр╕Бр╕▒р╕Ър╕Кр╕╕р╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕е
print(f"р╕Ир╕│р╕Щр╕зр╕Щр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф: {len(df)}")
print(f"р╕Ир╕│р╕Щр╕зр╕Щр╕кр╕▓р╕Вр╕▓р╕нр╕▓р╕Кр╕╡р╕Юр╕лр╕ер╕▒р╕Бр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф: {df['occupation_id'].nunique()}")
print("\nр╕Бр╕▓р╕гр╕Бр╕гр╕░р╕Ир╕▓р╕вр╕Вр╕нр╕Зр╕кр╕▓р╕Вр╕▓р╕нр╕▓р╕Кр╕╡р╕Юр╕лр╕ер╕▒р╕Б:")
occupation_distribution = df['occupation_id'].value_counts().sort_values(ascending=False)
for occupation_id, count in occupation_distribution.items():
    occupation_name = df[df['occupation_id'] == occupation_id]['р╕кр╕▓р╕Вр╕▓р╕нр╕▓р╕Кр╕╡р╕Юр╕лр╕ер╕▒р╕Б'].iloc[0] 
    print(f"- р╕гр╕лр╕▒р╕к {occupation_id} ({occupation_name}): {count} р╕гр╕▓р╕вр╕Бр╕▓р╕г ({count/len(df)*100:.2f}%)")

# ЁЯФ╣ р╣Бр╕Ър╣Ир╕Зр╕Др╕│р╕ар╕▓р╕йр╕▓р╣Др╕Чр╕вр╣Бр╕ер╕░р╕кр╕гр╣Йр╕▓р╕З word embedding
print("\nЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╣Бр╕Ър╣Ир╕Зр╕Др╕│р╣Бр╕ер╕░р╕кр╕гр╣Йр╕▓р╕З Word Embedding...")
df['tokenized'] = df['р╕Кр╕╖р╣Ир╕нр╕Хр╕│р╣Бр╕лр╕Щр╣Ир╕Зр╕Зр╕▓р╕Щ'].apply(lambda x: word_tokenize(x, engine='newmm'))

# р╕кр╕гр╣Йр╕▓р╕Зр╣Вр╕бр╣Ар╕Фр╕е Word2Vec
w2v_model = Word2Vec(sentences=df['tokenized'], vector_size=100, window=5, min_count=1, workers=4)

# ЁЯФ╣ р╕кр╕гр╣Йр╕▓р╕Зр╣Ар╕зр╕Бр╣Ар╕Хр╕нр╕гр╣Мр╕Ир╕▓р╕Бр╕Кр╕╖р╣Ир╕нр╕Хр╕│р╣Бр╕лр╕Щр╣Ир╕Зр╕Зр╕▓р╕Щ
def get_document_vector(tokens, model, vector_size=100):
    vec = np.zeros(vector_size)
    count = 0
    for word in tokens:
        try:
            vec += model.wv[word]
            count += 1
        except KeyError:
            continue
    if count != 0:
        vec /= count
    return vec

# р╕кр╕гр╣Йр╕▓р╕Зр╣Ар╕зр╕Бр╣Ар╕Хр╕нр╕гр╣Мр╕кр╕│р╕лр╕гр╕▒р╕Ър╣Бр╕Хр╣Ир╕ер╕░р╕Хр╕│р╣Бр╕лр╕Щр╣Ир╕Зр╕Зр╕▓р╕Щ
X_w2v = np.array([get_document_vector(tokens, w2v_model) for tokens in df['tokenized']])

# ЁЯФ╣ р╣Бр╕Ыр╕ер╕Зр╕Др╣Ир╕▓р╕лр╕бр╕зр╕Фр╕лр╕бр╕╣р╣Ир╣Ар╕Ыр╣Зр╕Щр╕Хр╕▒р╕зр╣Ар╕ер╕В
le_occupation = LabelEncoder()
df["occupation_id_encoded"] = le_occupation.fit_transform(df["occupation_id"])
y_main = df["occupation_id_encoded"]

# ЁЯФ╣ р╣Гр╕Кр╣Й TF-IDF р╣Ар╕Юр╕╖р╣Ир╕нр╣Бр╕Ыр╕ер╕Зр╕Кр╕╖р╣Ир╕нр╕Хр╕│р╣Бр╕лр╕Щр╣Ир╕Зр╕Зр╕▓р╕Щр╣Ар╕Ыр╣Зр╕Щр╣Ар╕зр╕Бр╣Ар╕Хр╕нр╕гр╣М
print("ЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╣Бр╕Ыр╕ер╕Зр╕Вр╣Йр╕нр╕Др╕зр╕▓р╕бр╣Ар╕Ыр╣Зр╕Щр╣Ар╕зр╕Бр╣Ар╕Хр╕нр╕гр╣Мр╕Фр╣Йр╕зр╕в TF-IDF...")
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df["р╕Кр╕╖р╣Ир╕нр╕Хр╕│р╣Бр╕лр╕Щр╣Ир╕Зр╕Зр╕▓р╕Щ"])

# ЁЯФ╣ р╣Бр╕кр╕Фр╕Зр╕Бр╕▓р╕гр╕Бр╕гр╕░р╕Ир╕▓р╕вр╕Вр╕нр╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Бр╣Ир╕нр╕Щр╕Ыр╕гр╕▒р╕Ър╕кр╕бр╕Фр╕╕р╕е
print(f"\nр╕Бр╕▓р╕гр╕Бр╕гр╕░р╕Ир╕▓р╕вр╕Вр╕нр╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Бр╣Ир╕нр╕Щр╕Ыр╕гр╕▒р╕Ър╕кр╕бр╕Фр╕╕р╕е: {Counter(y_main)}")

# ЁЯФ╣ р╣Гр╕Кр╣Й SMOTE р╣Ар╕Юр╕╖р╣Ир╕нр╕Ыр╕гр╕▒р╕Ър╕кр╕бр╕Фр╕╕р╕ер╕Вр╣Йр╕нр╕бр╕╣р╕е
print("ЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╕Ыр╕гр╕▒р╕Ър╕кр╕бр╕Фр╕╕р╕ер╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Фр╣Йр╕зр╕в SMOTE...")
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y_main)

# р╣Бр╕кр╕Фр╕Зр╕Бр╕▓р╕гр╕Бр╕гр╕░р╕Ир╕▓р╕вр╕Вр╕нр╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕лр╕ер╕▒р╕Зр╕Ыр╕гр╕▒р╕Ър╕кр╕бр╕Фр╕╕р╕е
print(f"р╕Бр╕▓р╕гр╕Бр╕гр╕░р╕Ир╕▓р╕вр╕Вр╕нр╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕лр╕ер╕▒р╕Зр╕Ыр╕гр╕▒р╕Ър╕кр╕бр╕Фр╕╕р╕е: {Counter(y_resampled)}")

# ЁЯФ╣ р╣Бр╕Ър╣Ир╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Ар╕Ыр╣Зр╕Щр╕Кр╕╕р╕Фр╕Эр╕╢р╕Бр╕кр╕нр╕Щр╣Бр╕ер╕░р╕Кр╕╕р╕Фр╕Чр╕Фр╕кр╕нр╕Ъ
print("\nЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╣Бр╕Ър╣Ир╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Ар╕Ыр╣Зр╕Щр╕Кр╕╕р╕Фр╕Эр╕╢р╕Бр╕кр╕нр╕Щр╣Бр╕ер╕░р╕Кр╕╕р╕Фр╕Чр╕Фр╕кр╕нр╕Ъ...")
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# ЁЯФ╣ р╕кр╕гр╣Йр╕▓р╕Зр╣Бр╕ер╕░р╕Эр╕╢р╕Бр╕кр╕нр╕Щр╣Вр╕бр╣Ар╕Фр╕е RandomForest
print("ЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╕кр╕гр╣Йр╕▓р╕Зр╣Бр╕ер╕░р╕Эр╕╢р╕Бр╕кр╕нр╕Щр╣Вр╕бр╣Ар╕Фр╕е RandomForest...")
rf_main = RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_split=5, min_samples_leaf=2, class_weight='balanced', random_state=42)

# ЁЯФ╣ р╕Ыр╕гр╕░р╣Ар╕бр╕┤р╕Щр╣Вр╕бр╣Ар╕Фр╕ер╕Фр╣Йр╕зр╕в Cross Validation
print("ЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╕Ыр╕гр╕░р╣Ар╕бр╕┤р╕Щр╣Вр╕бр╣Ар╕Фр╕ер╕Фр╣Йр╕зр╕в Cross Validation...")
cv_main = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_score_main = cross_val_score(rf_main, X_train, y_train, cv=cv_main)
print(f"ЁЯОп р╕Др╕зр╕▓р╕бр╣Бр╕бр╣Ир╕Щр╕вр╕│р╣Ар╕Йр╕ер╕╡р╣Ир╕вр╕Вр╕нр╕Зр╣Вр╕бр╣Ар╕Фр╕е (Cross Validation): {np.mean(cv_score_main):.4f}")

# ЁЯФ╣ р╕Эр╕╢р╕Бр╕кр╕нр╕Щр╣Вр╕бр╣Ар╕Фр╕ер╕Бр╕▒р╕Ър╕Кр╕╕р╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Эр╕╢р╕Бр╕кр╕нр╕Щр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф
print("ЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╕Эр╕╢р╕Бр╕кр╕нр╕Щр╣Вр╕бр╣Ар╕Фр╕ер╕Бр╕▒р╕Ър╕Кр╕╕р╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Эр╕╢р╕Бр╕кр╕нр╕Щр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф...")
rf_main.fit(X_train, y_train)

# ЁЯФ╣ р╕Чр╕Фр╕кр╕нр╕Ър╣Вр╕бр╣Ар╕Фр╕ер╕Бр╕▒р╕Ър╕Кр╕╕р╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Чр╕Фр╕кр╕нр╕Ъ
print("\nЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╕Чр╕Фр╕кр╕нр╕Ър╣Вр╕бр╣Ар╕Фр╕ер╕Бр╕▒р╕Ър╕Кр╕╕р╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Чр╕Фр╕кр╕нр╕Ъ...")
y_pred = rf_main.predict(X_test)
print("р╕гр╕▓р╕вр╕Зр╕▓р╕Щр╕Бр╕▓р╕гр╕Ир╕│р╣Бр╕Щр╕Бр╕Ыр╕гр╕░р╣Ар╕ар╕Ч:")
print(classification_report(y_test, y_pred))

# ЁЯФ╣ р╣Бр╕кр╕Фр╕Зр╕Др╕зр╕▓р╕бр╕кр╕│р╕Др╕▒р╕Нр╕Вр╕нр╕Зр╕Др╕╕р╕Ур╕ер╕▒р╕Бр╕йр╕Ур╕░
print("\nЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕Др╕зр╕▓р╕бр╕кр╕│р╕Др╕▒р╕Нр╕Вр╕нр╕Зр╕Др╕╕р╕Ур╕ер╕▒р╕Бр╕йр╕Ур╕░...")
feature_names = vectorizer.get_feature_names_out()
importances = rf_main.feature_importances_
indices = np.argsort(importances)[::-1]

print("20 р╕Др╕╕р╕Ур╕ер╕▒р╕Бр╕йр╕Ур╕░р╕Чр╕╡р╣Ир╕кр╕│р╕Др╕▒р╕Нр╕Чр╕╡р╣Ир╕кр╕╕р╕Ф:")
for i in range(min(20, len(indices))):
    print(f"{i+1}. {feature_names[indices[i]]} - {importances[indices[i]]:.4f}")

# ЁЯФ╣ р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Вр╕бр╣Ар╕Фр╕ер╣Бр╕ер╕░р╕кр╣Ир╕зр╕Щр╕Ыр╕гр╕░р╕Бр╕нр╕Ър╕кр╕│р╕Др╕▒р╕Н
print("\nЁЯФД р╕Бр╕│р╕ер╕▒р╕Зр╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Вр╕бр╣Ар╕Фр╕ер╣Бр╕ер╕░р╕кр╣Ир╕зр╕Щр╕Ыр╕гр╕░р╕Бр╕нр╕Ър╕кр╕│р╕Др╕▒р╕Н...")
os.makedirs('models', exist_ok=True)
# р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Вр╕бр╣Ар╕Фр╕е RandomForest
joblib.dump(rf_main, 'models/occupation_rf_model.joblib')
# р╕Ър╕▒р╕Щр╕Чр╕╢р╕Б TF-IDF Vectorizer
joblib.dump(vectorizer, 'models/tfidf_vectorizer.joblib')
# р╕Ър╕▒р╕Щр╕Чр╕╢р╕Б LabelEncoder
joblib.dump(le_occupation, 'models/label_encoder.joblib')
# р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Вр╕бр╣Ар╕Фр╕е Word2Vec
w2v_model.save('models/word2vec_model.bin')
# р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕кр╕│р╕Др╕▒р╕Нр╕Ир╕▓р╕Б DataFrame
occupation_mapping = df[['occupation_id', 'р╕кр╕▓р╕Вр╕▓р╕нр╕▓р╕Кр╕╡р╕Юр╕лр╕ер╕▒р╕Б']].drop_duplicates()
occupation_mapping.to_csv('models/occupation_mapping.csv', index=False)

print("тЬЕ р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Вр╕бр╣Ар╕Фр╕ер╣Бр╕ер╕░р╕кр╣Ир╕зр╕Щр╕Ыр╕гр╕░р╕Бр╕нр╕Ър╕кр╕│р╕Др╕▒р╕Нр╣Ар╕гр╕╡р╕вр╕Ър╕гр╣Йр╕нр╕вр╣Бр╕ер╣Йр╕з")
print("тЬЕ р╕Бр╕▓р╕гр╕Эр╕╢р╕Бр╕кр╕нр╕Щр╣Вр╕бр╣Ар╕Фр╕ер╣Ар╕кр╕гр╣Зр╕Ир╕кр╕бр╕Ър╕╣р╕гр╕Ур╣М")